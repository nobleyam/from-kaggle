{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2564,"databundleVersionId":29456,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# read the data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/DontGetKicked/training.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:02.412474Z","iopub.execute_input":"2025-01-03T20:27:02.412893Z","iopub.status.idle":"2025-01-03T20:27:03.426567Z","shell.execute_reply.started":"2025-01-03T20:27:02.412856Z","shell.execute_reply":"2025-01-03T20:27:03.425300Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:04.980258Z","iopub.execute_input":"2025-01-03T20:27:04.980696Z","iopub.status.idle":"2025-01-03T20:27:04.986041Z","shell.execute_reply.started":"2025-01-03T20:27:04.980659Z","shell.execute_reply":"2025-01-03T20:27:04.984741Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Set 'RefId' as the index in df first\ndf = df.set_index('RefId')\n\n# Then define target and inputs\ntarget = df['IsBadBuy']\ninputs = df.drop(columns=['IsBadBuy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:05.886524Z","iopub.execute_input":"2025-01-03T20:27:05.887001Z","iopub.status.idle":"2025-01-03T20:27:05.945026Z","shell.execute_reply.started":"2025-01-03T20:27:05.886959Z","shell.execute_reply":"2025-01-03T20:27:05.943902Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# feature screening","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.30, random_state=42)\n\nX_train.shape,X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:08.416340Z","iopub.execute_input":"2025-01-03T20:27:08.416784Z","iopub.status.idle":"2025-01-03T20:27:09.242998Z","shell.execute_reply.started":"2025-01-03T20:27:08.416743Z","shell.execute_reply":"2025-01-03T20:27:09.241733Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((51088, 32), (21895, 32))"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def feature_screening(data, min_cv=0.1, mode_threshold=99, distinct_threshold=90):\n    processed_data = data.copy()\n    \n\n    columns = inputs.columns\n\n    continuous = [\n        'VehYear', 'VehicleAge', 'VehOdo', \n        'MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n        'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n        'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n        'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice',\n        'VehBCost', 'WarrantyCost'\n    ]\n    categorical = [j for j in columns if j not in continuous]\n\n    # Define a minimum value for coefficient of variation\n    min_cv = min_cv\n\n    # Calculate the coefficient of variation for each column\n    cv_values = processed_data[continuous].std() / processed_data[continuous].mean()\n\n    # Filter out columns with CV less than 0.1\n    screen_cv =  cv_values[cv_values < min_cv].index.tolist()\n\n\n    # Define a threshold for the dominant category percentage\n    mode_threshold = mode_threshold\n\n    # Calculate the percentage of the mode category for each column\n    mode_category = (processed_data[categorical].apply(lambda x: x.value_counts().max() / len(x)) * 100)\n\n    # Select columns where the mode category percentage is greater than the threshold\n    screen_mode = mode_category[mode_category > mode_threshold].index.tolist()\n\n\n    # Set a threshold for excluding columns \n    distinct_threshold = distinct_threshold\n\n    # Calculate the percentage of distinct categories in categorical variables\n    distinct_percentage = (processed_data[categorical].apply(lambda x: x.dropna().nunique() / x.count()) * 100)\n\n    # Select categorical columns based on distinct percentage threshold\n    screen_distinct = distinct_percentage[distinct_percentage > distinct_threshold].index.tolist()\n\n    screened_features  = list(set(screen_cv + screen_mode + screen_distinct))\n     \n    return screened_features ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:14.217840Z","iopub.execute_input":"2025-01-03T20:27:14.218324Z","iopub.status.idle":"2025-01-03T20:27:14.228046Z","shell.execute_reply.started":"2025-01-03T20:27:14.218267Z","shell.execute_reply":"2025-01-03T20:27:14.226746Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"drop_list = feature_screening(X_train, min_cv=0.1, mode_threshold=99, distinct_threshold=90)\n\nX_train = X_train.drop(drop_list, axis=1)\nX_test = X_test.drop(drop_list, axis=1)\n\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:22.672880Z","iopub.execute_input":"2025-01-03T20:27:22.673287Z","iopub.status.idle":"2025-01-03T20:27:22.982616Z","shell.execute_reply.started":"2025-01-03T20:27:22.673250Z","shell.execute_reply":"2025-01-03T20:27:22.981378Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((51088, 31), (21895, 31))"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Handle Out-of-Range ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef range_consistency(data, target):\n    # Define ranges for each column\n    column_ranges = {\n    'VehicleAge': (0,30),\n    'VehOdo': (0,120000),\n    'MMRAcquisitionAuctionAveragePrice': (800,46000),\n    'MMRAcquisitionAuctionCleanPrice': (1000,46000),\n    'MMRAcquisitionRetailAveragePrice': (1000,46000),\n    'MMRAcquisitonRetailCleanPrice': (1000,46000),\n    'MMRCurrentAuctionAveragePrice': (300,46000),\n    'MMRCurrentAuctionCleanPrice': (400,46000),\n    'MMRCurrentRetailAveragePrice': (800,46000),\n    'MMRCurrentRetailCleanPrice': (1000,46000),\n    'VehBCost': (1000,46000),\n    'WarrantyCost': (400,8000)\n    }\n\n    # Iterate through each column and fill NaN values outside the defined range\n    for column, (min_val, max_val) in column_ranges.items():\n        data[column] = data[column].apply(lambda x: x if min_val <= x <= max_val else None)\n        \n    return data, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:26.272694Z","iopub.execute_input":"2025-01-03T20:27:26.273682Z","iopub.status.idle":"2025-01-03T20:27:26.281222Z","shell.execute_reply.started":"2025-01-03T20:27:26.273635Z","shell.execute_reply":"2025-01-03T20:27:26.279877Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train = range_consistency(X_train, y_train)[0]\nX_test = range_consistency(X_test, y_test)[0]\n\ny_train = range_consistency(X_train, y_train)[1]\ny_test = range_consistency(X_test, y_test)[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:28.649298Z","iopub.execute_input":"2025-01-03T20:27:28.649756Z","iopub.status.idle":"2025-01-03T20:27:29.262379Z","shell.execute_reply.started":"2025-01-03T20:27:28.649719Z","shell.execute_reply":"2025-01-03T20:27:29.261118Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import numpy as np\n\ndef initial_preproc(data):\n    processed_data = data.copy()\n    # List of columns to drop\n    columns_to_drop = ['PurchDate', 'Model', 'Trim', 'SubModel', 'BYRNO', 'VNZIP1', 'VNST', 'WheelTypeID']\n    \n    # Drop the columns\n    processed_data = processed_data.drop(columns=columns_to_drop)\n    # Replace 'NOT AVAIL' in the 'Color' column with NaN\n    processed_data['Color'] = processed_data['Color'].replace('NOT AVAIL', np.nan)\n    processed_data['Transmission'] = processed_data['Transmission'].replace(['Manual'], 'MANUAL')\n    processed_data['Nationality'] = processed_data['Nationality'].replace(['OTHER','TOP LINE ASIAN'], 'OTHERS')\n    # Define a threshold for frequency (1% of total data)\n    threshold = 0.01 * len(processed_data)\n    \n    # Group classes in 'Color' column\n    color_freq = processed_data['Color'].value_counts()  # Get frequencies\n    color_to_other = color_freq[color_freq < threshold].index  # Classes with less than 1% frequency\n    processed_data['Color'] = processed_data['Color'].replace(color_to_other, 'OTHER')\n    \n    # Group classes in 'Make' column\n    make_freq = processed_data['Make'].value_counts()  # Get frequencies\n    make_to_other = make_freq[make_freq < threshold].index  # Classes with less than 1% frequency\n    processed_data['Make'] = processed_data['Make'].replace(make_to_other, 'OTHER')\n    \n    #From our previous Chi-Square Test\n    processed_data['PRIMEUNIT'].fillna('unknown', inplace=True)\n    processed_data['AUCGUART'].fillna('unknown', inplace=True)\n    \n    \n\n\n\n    return processed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:31.834506Z","iopub.execute_input":"2025-01-03T20:27:31.835338Z","iopub.status.idle":"2025-01-03T20:27:31.844237Z","shell.execute_reply.started":"2025-01-03T20:27:31.835272Z","shell.execute_reply":"2025-01-03T20:27:31.843087Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"X_train = initial_preproc(X_train)\nX_test = initial_preproc(X_test)\n\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:34.799811Z","iopub.execute_input":"2025-01-03T20:27:34.800265Z","iopub.status.idle":"2025-01-03T20:27:34.998380Z","shell.execute_reply.started":"2025-01-03T20:27:34.800224Z","shell.execute_reply":"2025-01-03T20:27:34.996993Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"((51088, 23), (21895, 23))"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# outliers","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\ndef outlier_handling_replace(data, contamination=0.01):\n    inputs_iso = data.copy()\n    \n    # Continuous variables\n    continuous_columns = [\n        'VehicleAge', 'VehOdo', 'MMRAcquisitionAuctionAveragePrice', \n        'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice', \n        'MMRAcquisitonRetailCleanPrice', 'MMRCurrentAuctionAveragePrice', \n        'MMRCurrentAuctionCleanPrice', 'MMRCurrentRetailAveragePrice', \n        'MMRCurrentRetailCleanPrice', 'VehBCost', 'WarrantyCost'\n    ]\n    \n    # Categorical variables\n    categorical_columns = [\n        'Auction', 'Color', 'Transmission', 'WheelType', \n        'Nationality', 'Size', 'TopThreeAmericanName', \n        'PRIMEUNIT', 'AUCGUART', 'IsOnlineSale', 'Make'\n    ]\n    \n    # Discard rows with NaN values\n    inputs_iso = inputs_iso.dropna()\n    \n    # Apply Z-score scaling to numerical columns\n    scaler = StandardScaler()\n    inputs_iso[continuous_columns] = scaler.fit_transform(inputs_iso[continuous_columns])\n    \n    # Apply label encoding to categorical columns\n    label_encoder = LabelEncoder()\n    inputs_iso[categorical_columns] = inputs_iso[categorical_columns].apply(label_encoder.fit_transform)\n    \n    # Fit Isolation Forest model\n    clf = IsolationForest(contamination=contamination, random_state=42)\n    clf.fit(inputs_iso)\n    \n    # Predict outliers\n    outliers = clf.predict(inputs_iso)\n    \n    # Add the outlier predictions to your DataFrame\n    inputs_iso['outlier'] = outliers\n    \n    # Identify outliers\n    outlier_index = inputs_iso[inputs_iso['outlier'] == -1].index\n    \n    # Replace outliers with median for continuous columns\n    for col in continuous_columns:\n        median_value = data[col].median()\n        data.loc[outlier_index, col] = median_value\n    \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:37.474575Z","iopub.execute_input":"2025-01-03T20:27:37.475049Z","iopub.status.idle":"2025-01-03T20:27:37.744793Z","shell.execute_reply.started":"2025-01-03T20:27:37.475006Z","shell.execute_reply":"2025-01-03T20:27:37.743645Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"X_train_cleaned = outlier_handling_replace(X_train, contamination=0.01)\n\nX_train_cleaned.shape, y_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:40.525685Z","iopub.execute_input":"2025-01-03T20:27:40.526127Z","iopub.status.idle":"2025-01-03T20:27:44.090719Z","shell.execute_reply.started":"2025-01-03T20:27:40.526087Z","shell.execute_reply":"2025-01-03T20:27:44.089642Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"((51088, 23), (51088,))"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# missing values","metadata":{}},{"cell_type":"markdown","source":"## Row cleaning","metadata":{}},{"cell_type":"code","source":"def missing_row_report(data, missrow=12):\n    processed_data = data.copy()\n\n    # Create a new column with the number of missing values in each row\n    processed_data['Num_Missing_Values'] = processed_data.isnull().sum(axis=1)\n\n    discard_missing_row = processed_data[processed_data['Num_Missing_Values'] > missrow].index.tolist()\n\n    return discard_missing_row","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:49.305902Z","iopub.execute_input":"2025-01-03T20:27:49.306364Z","iopub.status.idle":"2025-01-03T20:27:49.312812Z","shell.execute_reply.started":"2025-01-03T20:27:49.306325Z","shell.execute_reply":"2025-01-03T20:27:49.311412Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"discard_missing_row = missing_row_report(X_train, missrow=35)\n\nX_train = X_train.drop(discard_missing_row)\ny_train = y_train.drop(discard_missing_row)\n\nX_train.shape, y_train.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:51.202761Z","iopub.execute_input":"2025-01-03T20:27:51.203155Z","iopub.status.idle":"2025-01-03T20:27:51.263707Z","shell.execute_reply.started":"2025-01-03T20:27:51.203121Z","shell.execute_reply":"2025-01-03T20:27:51.262576Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"((51088, 23), (51088,))"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## column cleaning ","metadata":{}},{"cell_type":"code","source":"def missing_col_report(data, misscol=50):\n    processed_data = data.copy()\n    \n    # Report on count and percentage of missing values in each column\n    missing_values_report = pd.DataFrame({\n        'Column': processed_data.columns,\n        'Missing Values': processed_data.isnull().sum(),\n        'Percentage Missing': processed_data.isnull().mean() * 100\n        })\n    discard_missing_col = missing_values_report[missing_values_report['Percentage Missing'] > misscol].index.tolist()\n    \n    return discard_missing_col","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:54.106556Z","iopub.execute_input":"2025-01-03T20:27:54.107007Z","iopub.status.idle":"2025-01-03T20:27:54.114189Z","shell.execute_reply.started":"2025-01-03T20:27:54.106968Z","shell.execute_reply":"2025-01-03T20:27:54.112614Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"discard_missing_col = missing_col_report(X_train, misscol=50)\n\nX_train = X_train.drop(discard_missing_col, axis=1)\nX_test = X_test.drop(discard_missing_col, axis=1)\n\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:56.534467Z","iopub.execute_input":"2025-01-03T20:27:56.534917Z","iopub.status.idle":"2025-01-03T20:27:56.616480Z","shell.execute_reply.started":"2025-01-03T20:27:56.534878Z","shell.execute_reply":"2025-01-03T20:27:56.615155Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"((51088, 23), (21895, 23))"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## impute missing values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import  SimpleImputer\n\ndef missing_imputer(train, test):\n    \n    continuous = train.select_dtypes(exclude=['object','category']).columns.tolist()\n    categorical = train.select_dtypes(include=['object','category']).columns.tolist()\n\n    # Define imputation strategies for each subset of columns\n    cat_imputer = SimpleImputer(strategy='most_frequent')\n    cont_imputer = SimpleImputer(strategy='median')\n    \n    try:\n\n    # Impute missing values\n        train[continuous] = cont_imputer.fit_transform(train[continuous])\n        train[categorical] = cat_imputer.fit_transform(train[categorical])\n    \n        test[continuous] = cont_imputer.transform(test[continuous])\n        test[categorical] = cat_imputer.transform(test[categorical])\n\n    except:\n        test[continuous] = cont_imputer.transform(test[continuous])\n        test[categorical] = cat_imputer.transform(test[categorical])\n        \n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:27:59.217238Z","iopub.execute_input":"2025-01-03T20:27:59.218526Z","iopub.status.idle":"2025-01-03T20:27:59.234572Z","shell.execute_reply.started":"2025-01-03T20:27:59.218474Z","shell.execute_reply":"2025-01-03T20:27:59.233349Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"X_train, X_test = missing_imputer(X_train, X_test)\n\nX_train.shape, X_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:01.750681Z","iopub.execute_input":"2025-01-03T20:28:01.751293Z","iopub.status.idle":"2025-01-03T20:28:02.061283Z","shell.execute_reply.started":"2025-01-03T20:28:01.751237Z","shell.execute_reply":"2025-01-03T20:28:02.059818Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"((51088, 23), (21895, 23))"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"# transformation using the Box-Cox","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.preprocessing import PowerTransformer\nimport pandas as pd\n\ndef transform_features(data):\n   \n    data_transformed = data.copy()  # Avoid modifying the original DataFrame\n    features = [\"VehBCost\", \"WarrantyCost\"]\n\n    for feature in features:\n        # Check if the feature contains non-positive values\n        has_negative_values = (data[feature] <= 0).any()\n        \n        # Select transformation method based on values\n        method = 'yeo-johnson' if has_negative_values else 'box-cox'\n        transformer = PowerTransformer(method=method, standardize=False)\n        \n        # Fit and transform the feature\n        data_transformed[f\"{feature}_transformed\"] = transformer.fit_transform(data[[feature]])\n        # Drop the original columns used for PowerTransformer\n        \n\n    return data_transformed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:05.873524Z","iopub.execute_input":"2025-01-03T20:28:05.874565Z","iopub.status.idle":"2025-01-03T20:28:05.881863Z","shell.execute_reply.started":"2025-01-03T20:28:05.874524Z","shell.execute_reply":"2025-01-03T20:28:05.880385Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"transformed_train = transform_features(X_train)\ntransformed_test = transform_features(X_test)\n\ntransformed_train.shape, transformed_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:08.667770Z","iopub.execute_input":"2025-01-03T20:28:08.668184Z","iopub.status.idle":"2025-01-03T20:28:09.659385Z","shell.execute_reply.started":"2025-01-03T20:28:08.668148Z","shell.execute_reply":"2025-01-03T20:28:09.658164Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"((51088, 25), (21895, 25))"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\n\n\n# continuous variables\ncontinuous_columns_trans = [\n        'VehicleAge', 'VehOdo', 'MMRAcquisitionAuctionAveragePrice', \n        'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice', \n        'MMRAcquisitonRetailCleanPrice', 'MMRCurrentAuctionAveragePrice', \n        'MMRCurrentAuctionCleanPrice', 'MMRCurrentRetailAveragePrice', \n        'MMRCurrentRetailCleanPrice','VehBCost_transformed','WarrantyCost_transformed']\n    \n# Categorical variables\ncategorical_columns_trans = [\n        'Auction', 'Color', 'Transmission', 'WheelType', \n        'Nationality', 'Size', 'TopThreeAmericanName', \n        'PRIMEUNIT', 'AUCGUART', 'IsOnlineSale', 'Make' ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:12.104733Z","iopub.execute_input":"2025-01-03T20:28:12.105170Z","iopub.status.idle":"2025-01-03T20:28:12.111824Z","shell.execute_reply.started":"2025-01-03T20:28:12.105133Z","shell.execute_reply":"2025-01-03T20:28:12.110244Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import  OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n\n\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n\nordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n\nz_score = StandardScaler()\n\nlda = LinearDiscriminantAnalysis(n_components=None)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:14.475145Z","iopub.execute_input":"2025-01-03T20:28:14.476525Z","iopub.status.idle":"2025-01-03T20:28:14.696453Z","shell.execute_reply.started":"2025-01-03T20:28:14.476470Z","shell.execute_reply":"2025-01-03T20:28:14.694986Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"!pip install lightgbm --upgrade -q\n!pip install --no-cache-dir lightgbm -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T13:58:04.836630Z","iopub.execute_input":"2025-01-03T13:58:04.837518Z","iopub.status.idle":"2025-01-03T13:58:21.488656Z","shell.execute_reply.started":"2025-01-03T13:58:04.837479Z","shell.execute_reply":"2025-01-03T13:58:21.487402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport lightgbm as lgb\n\n# Parameters for LightGBM\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=1000,           # Number of boosting rounds\n    learning_rate=0.01,          # Learning rate\n    max_depth=-1,                # No limit on the depth of trees (can be tuned)\n    num_leaves=31,               # The number of leaves in one tree (tuneable)\n    subsample=0.8,               # Fraction of samples to use for each tree\n    colsample_bytree=0.8,        # Fraction of features to use for each tree\n    random_state=8,              # Random state for reproducibility\n    is_unbalance=False,           # Helps to handle class imbalance if needed\n    scale_pos_weight=3           # Used for handling imbalanced data (adjust if needed)\n)\n\nwrapper = RFECV(estimator=DecisionTreeClassifier(random_state=29), step=1, min_features_to_select=10, cv=5, n_jobs=-1)\n\n# Preprocessing steps\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\nz_score = StandardScaler()\n\n# Define preprocessing for numerical and categorical features\nnumerical_preprocessing = Pipeline(steps=[\n    ('scaler', z_score),  # Scale numerical features\n    ('LDA', lda)\n])\n\nnominal_preprocessing = Pipeline(steps=[\n    ('encoder', one_hot_encoder),  # One-hot encode nominal features\n    ('scaler', z_score)            # Scale encoded features\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_preprocessing, continuous_columns_trans),  # Replace with your numerical column names\n    ('nom', nominal_preprocessing, categorical_columns_trans)    # Replace with your categorical column names\n], remainder='drop')  # Drop any columns not specified\n\n# Define the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('wrapper', wrapper),\n    ('model', lgb_model)  # Use LightGBM model\n])\n\n# Fit the pipeline with original (non-resampled) data\npipe = pipeline.fit(transformed_train, y_train)\n\n# Predict on the training set\ny_pred_train = pipe.predict(transformed_train)\n\n# Predict on the testing set\ny_pred_test = pipe.predict(transformed_test)\n\n# Evaluate the model on the training set\ncm_train = confusion_matrix(y_train, y_pred_train)\nreport_train = classification_report(y_train, y_pred_train)\n\n# Evaluate the model on the testing set\ncm_test = confusion_matrix(y_test, y_pred_test)\nreport_test = classification_report(y_test, y_pred_test)\n\n# Print the evaluation results\nprint(\"Evaluation of the Model on Training Set\")\nprint(f\"Confusion Matrix:\\n{cm_train}\")\nprint(f\"Classification Report:\\n{report_train}\")\nprint(\"-\" * 80)\nprint(\"Evaluation of the Model on Testing Set\")\nprint(f\"Confusion Matrix:\\n{cm_test}\")\nprint(f\"Classification Report:\\n{report_test}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:28:19.960504Z","iopub.execute_input":"2025-01-03T20:28:19.960943Z","iopub.status.idle":"2025-01-03T20:29:17.741500Z","shell.execute_reply.started":"2025-01-03T20:28:19.960904Z","shell.execute_reply":"2025-01-03T20:29:17.740064Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 6293, number of negative: 44795\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012222 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 348\n[LightGBM] [Info] Number of data points in the train set: 51088, number of used features: 32\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.123180 -> initscore=-1.962659\n[LightGBM] [Info] Start training from score -1.962659\nEvaluation of the Model on Training Set\nConfusion Matrix:\n[[42349  2446]\n [ 4744  1549]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.95      0.92     44795\n           1       0.39      0.25      0.30      6293\n\n    accuracy                           0.86     51088\n   macro avg       0.64      0.60      0.61     51088\nweighted avg       0.84      0.86      0.85     51088\n\n--------------------------------------------------------------------------------\nEvaluation of the Model on Testing Set\nConfusion Matrix:\n[[ 6240 12972]\n [  360  2323]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.32      0.48     19212\n           1       0.15      0.87      0.26      2683\n\n    accuracy                           0.39     21895\n   macro avg       0.55      0.60      0.37     21895\nweighted avg       0.85      0.39      0.46     21895\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport lightgbm as lgb\n\n# Parameters for LightGBM\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=1000,           # Number of boosting rounds\n    learning_rate=0.01,          # Learning rate\n    max_depth=-1,                # No limit on the depth of trees\n    num_leaves=31,               # The number of leaves in one tree\n    subsample=0.8,               # Fraction of samples to use for each tree\n    colsample_bytree=0.8,        # Fraction of features to use for each tree\n    random_state=8,              # Random state for reproducibility\n    is_unbalance=False,          # Helps to handle class imbalance if needed\n    scale_pos_weight=3           # Used for handling imbalanced data\n)\n\n# Define RFECV with RandomForestClassifier as the estimator\nwrapper = RFECV(\n    estimator=RandomForestClassifier(random_state=29, n_jobs=-1), \n    step=1, \n    min_features_to_select=10, \n    cv=5, \n    n_jobs=-1\n)\n\n# Preprocessing steps\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\nz_score = StandardScaler()\nlda = LinearDiscriminantAnalysis()  # LDA as a dimensionality reduction technique\n\n# Define preprocessing for numerical and categorical features\nnumerical_preprocessing = Pipeline(steps=[\n    ('scaler', z_score),  # Scale numerical features\n    ('LDA', lda)           # Apply LDA for dimensionality reduction\n])\n\nnominal_preprocessing = Pipeline(steps=[\n    ('encoder', one_hot_encoder),  # One-hot encode nominal features\n    ('scaler', z_score)            # Scale encoded features\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_preprocessing, continuous_columns_trans),  # Replace with your numerical column names\n    ('nom', nominal_preprocessing, categorical_columns_trans)    # Replace with your categorical column names\n], remainder='drop')  # Drop any columns not specified\n\n# Define the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('wrapper', wrapper),          # RFECV with RandomForestClassifier\n    ('model', lgb_model)           # Use LightGBM model\n])\n\n# Fit the pipeline with original (non-resampled) data\npipe = pipeline.fit(transformed_train, y_train)\n\n# Predict on the training set\ny_pred_train = pipe.predict(transformed_train)\n\n# Predict on the testing set\ny_pred_test = pipe.predict(transformed_test)\n\n# Evaluate the model on the training set\ncm_train = confusion_matrix(y_train, y_pred_train)\nreport_train = classification_report(y_train, y_pred_train)\n\n# Evaluate the model on the testing set\ncm_test = confusion_matrix(y_test, y_pred_test)\nreport_test = classification_report(y_test, y_pred_test)\n\n# Print the evaluation results\nprint(\"Evaluation of the Model on Training Set\")\nprint(f\"Confusion Matrix:\\n{cm_train}\")\nprint(f\"Classification Report:\\n{report_train}\")\nprint(\"-\" * 80)\nprint(\"Evaluation of the Model on Testing Set\")\nprint(f\"Confusion Matrix:\\n{cm_test}\")\nprint(f\"Classification Report:\\n{report_test}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\n\n# Preprocessing steps\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\nz_score = StandardScaler()\n\n# Define preprocessing for numerical and categorical features\nnumerical_preprocessing = Pipeline(steps=[\n    ('scaler', z_score),  # Scale numerical features\n])\n\nnominal_preprocessing = Pipeline(steps=[\n    ('encoder', one_hot_encoder),  # One-hot encode nominal features\n    ('scaler', z_score)            # Scale encoded features\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_preprocessing, continuous_columns_trans),  # Replace with your numerical column names\n    ('nom', nominal_preprocessing, categorical_columns_trans)    # Replace with your categorical column names\n], remainder='drop')  # Drop any columns not specified\n\n# Define the LightGBM model with GPU\nlgb_model = lgb.LGBMClassifier(\n    random_state=8,\n    device='gpu'  # Enable GPU support\n)\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    'model__n_estimators': [ 1000, 1500],  # Number of boosting rounds\n    'model__learning_rate': [0.01, 0.05],  # Learning rate\n    'model__max_depth': [ 8, 10, 12 ],  # Tree depth\n    'model__num_leaves': [ 31, 50],  # Number of leaves\n    'model__subsample': [0.7, 0.8, ],  # Fraction of samples\n    'model__colsample_bytree': [0.7, 0.8],  # Fraction of features\n    'model__scale_pos_weight': [  3, 4, 5]  # Class imbalance handling\n}\n\n# Define the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', lgb_model)  # Use LightGBM model\n])\n\n# Setup the GridSearchCV\ngrid_search = GridSearchCV(\n    pipeline, param_grid, cv=3, verbose=1, n_jobs=-1, scoring='accuracy'\n)\n\n# Fit the model using GridSearchCV\ngrid_search.fit(transformed_train, y_train)\n\n# Print the best parameters found by GridSearchCV\nprint(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n\n# Get the best model from the grid search\nbest_model = grid_search.best_estimator_\n\n# Predict on the training set\ny_pred_train = best_model.predict(transformed_train)\n\n# Predict on the testing set\ny_pred_test = best_model.predict(transformed_test)\n\n# Evaluate the model on the training set\ncm_train = confusion_matrix(y_train, y_pred_train)\nreport_train = classification_report(y_train, y_pred_train)\n\n# Evaluate the model on the testing set\ncm_test = confusion_matrix(y_test, y_pred_test)\nreport_test = classification_report(y_test, y_pred_test)\n\n# Print the evaluation results\nprint(\"Evaluation of the Model on Training Set\")\nprint(f\"Confusion Matrix:\\n{cm_train}\")\nprint(f\"Classification Report:\\n{report_train}\")\nprint(\"-\" * 80)\nprint(\"Evaluation of the Model on Testing Set\")\nprint(f\"Confusion Matrix:\\n{cm_test}\")\nprint(f\"Classification Report:\\n{report_test}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T14:44:37.899733Z","iopub.execute_input":"2025-01-03T14:44:37.900550Z","iopub.status.idle":"2025-01-03T15:19:29.409027Z","shell.execute_reply.started":"2025-01-03T14:44:37.900512Z","shell.execute_reply":"2025-01-03T15:19:29.407312Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Parameters for Logistic Regression\nlog_reg_model = LogisticRegression(\n    C=0.1,\n    l1_ratio=0.25,\n    penalty='elasticnet',\n    solver='saga',\n    random_state=8\n)\n\n# Preprocessing steps\none_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\nz_score = StandardScaler()\nlda = LinearDiscriminantAnalysis()\n\n# Define preprocessing for numerical and categorical features\nnumerical_preprocessing = Pipeline(steps=[\n    ('scaler', z_score),  # Scale numerical features\n    ('LDA', lda)          # Feature extraction on numerical features\n])\n\nnominal_preprocessing = Pipeline(steps=[\n    ('encoder', one_hot_encoder),  # One-hot encode nominal features\n    ('scaler', z_score)            # Scale encoded features\n])\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_preprocessing, continuous_columns_trans),  # Replace with your numerical column names\n    ('nom', nominal_preprocessing, categorical_columns_trans)    # Replace with your categorical column names\n], remainder='drop')  # Drop any columns not specified\n\n# Define the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', log_reg_model)  # Use Logistic Regression model\n])\n\n# Apply RandomOverSampler to balance the dataset\nros = RandomOverSampler(random_state=8)\nX_resampled, y_resampled = ros.fit_resample(transformed_train, y_train)\n\n# Fit the pipeline with resampled data\npipe = pipeline.fit(X_resampled, y_resampled)\n\n# Predict on the training set\ny_pred_train = pipe.predict(transformed_train)\n\n# Predict on the testing set\ny_pred_test = pipe.predict(transformed_test)\n\n# Evaluate the model on the training set\ncm_train = confusion_matrix(y_train, y_pred_train)\nreport_train = classification_report(y_train, y_pred_train)\n\n# Evaluate the model on the testing set\ncm_test = confusion_matrix(y_test, y_pred_test)\nreport_test = classification_report(y_test, y_pred_test)\n\n# Print the evaluation results\nprint(\"Evaluation of the Model on Training Set\")\nprint(f\"Confusion Matrix:\\n{cm_train}\")\nprint(f\"Classification Report:\\n{report_train}\")\nprint(\"-\"*80)\nprint(\"Evaluation of the Model on Testing Set\")\nprint(f\"Confusion Matrix:\\n{cm_test}\")\nprint(f\"Classification Report:\\n{report_test}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T12:01:05.869546Z","iopub.status.idle":"2025-01-03T12:01:05.869881Z","shell.execute_reply.started":"2025-01-03T12:01:05.869722Z","shell.execute_reply":"2025-01-03T12:01:05.869743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikeras -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T12:01:05.871163Z","iopub.status.idle":"2025-01-03T12:01:05.871487Z","shell.execute_reply.started":"2025-01-03T12:01:05.871338Z","shell.execute_reply":"2025-01-03T12:01:05.871355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U imbalanced-learn -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T12:01:05.872314Z","iopub.status.idle":"2025-01-03T12:01:05.872605Z","shell.execute_reply.started":"2025-01-03T12:01:05.872470Z","shell.execute_reply":"2025-01-03T12:01:05.872484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scikeras.wrappers import KerasClassifier  # Use SciKeras instead of keras.wrappers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport random\n\nStandardScaler(with_mean=False)\n\n# Seed for reproducibility\nseed_value = 3\nrandom.seed(seed_value)\n\n# Define the neural network model\ndef create_model(input_shape):\n    model = Sequential()\n    model.add(Dense(units=80, activation='sigmoid', input_shape=(input_shape,)))\n    model.add(Dense(units=64, activation='sigmoid'))\n    model.add(Dense(units=16, activation='sigmoid'))\n    model.add(Dense(units=48, activation='sigmoid'))\n    model.add(Dense(units=1, activation='sigmoid'))\n    model.compile(optimizer=Adam(learning_rate=0.001), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])  # Use 'accuracy' instead of 'f1_score'\n    return model\n\n# Define preprocessing for numerical and categorical features\n# Define preprocessing for numerical and categorical features\nnumerical_preprocessing = Pipeline(steps=[\n    ('scaler', StandardScaler(with_mean=False)),  # Set with_mean=False\n    ('lda', LDA())  # Feature extraction on numerical features\n])\n\nnominal_preprocessing = Pipeline(steps=[\n    ('encoder', OneHotEncoder()),  # One-hot encode nominal features\n    ('scaler', StandardScaler(with_mean=False))  # Scale encoded features\n])\n\n\n# Combine preprocessing\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_preprocessing, continuous_columns_trans),  # Replace with your numerical column names\n    ('nom', nominal_preprocessing, categorical_columns_trans)    # Replace with your categorical column names\n], remainder='drop')  # Drop any columns not specified\n\n# Define the full pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', KerasClassifier(model=create_model, input_shape=transformed_train.shape[1], epochs=100, batch_size=20, validation_split=0.2, verbose=1))  # Use SciKeras wrapper\n])\n\n# Fit the pipeline with the original training data\npipeline.fit(transformed_train, y_train)\n\n# Predict on the training set\ny_pred_train = (pipeline.predict(transformed_train) > 0.5).astype(int)\n\n# Predict on the testing set\ny_pred_test = (pipeline.predict(transformed_test) > 0.5).astype(int)\n\n# Evaluate the model on the training set\ncm_train = confusion_matrix(y_train, y_pred_train)\nreport_train = classification_report(y_train, y_pred_train)\n\n# Evaluate the model on the testing set\ncm_test = confusion_matrix(y_test, y_pred_test)\nreport_test = classification_report(y_test, y_pred_test)\n\n# Print the evaluation results\nprint(\"Evaluation of the Model on Training Set\")\nprint(f\"Confusion Matrix:\\n{cm_train}\")\nprint(f\"Classification Report:\\n{report_train}\")\nprint(\"-\" * 80)\nprint(\"Evaluation of the Model on Testing Set\")\nprint(f\"Confusion Matrix:\\n{cm_test}\")\nprint(f\"Classification Report:\\n{report_test}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T12:01:05.875354Z","iopub.status.idle":"2025-01-03T12:01:05.875655Z","shell.execute_reply.started":"2025-01-03T12:01:05.875512Z","shell.execute_reply":"2025-01-03T12:01:05.875527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/DontGetKicked/test.csv')\ntest.set_index('RefId', inplace=True)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:29:31.509775Z","iopub.execute_input":"2025-01-03T20:29:31.510754Z","iopub.status.idle":"2025-01-03T20:29:31.947677Z","shell.execute_reply.started":"2025-01-03T20:29:31.510709Z","shell.execute_reply":"2025-01-03T20:29:31.946284Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(48707, 32)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"test = initial_preproc(test)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:29:32.946492Z","iopub.execute_input":"2025-01-03T20:29:32.946864Z","iopub.status.idle":"2025-01-03T20:29:33.048550Z","shell.execute_reply.started":"2025-01-03T20:29:32.946833Z","shell.execute_reply":"2025-01-03T20:29:33.047358Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(48707, 24)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"test = test.drop(drop_list, axis=1)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:29:34.318854Z","iopub.execute_input":"2025-01-03T20:29:34.319245Z","iopub.status.idle":"2025-01-03T20:29:34.334567Z","shell.execute_reply.started":"2025-01-03T20:29:34.319212Z","shell.execute_reply":"2025-01-03T20:29:34.333389Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test = range_consistency( X_train, test)[1]\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:29:39.371685Z","iopub.execute_input":"2025-01-03T20:29:39.372159Z","iopub.status.idle":"2025-01-03T20:29:39.588483Z","shell.execute_reply.started":"2025-01-03T20:29:39.372113Z","shell.execute_reply":"2025-01-03T20:29:39.587227Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"test = outlier_handling_replace(test, contamination=0.01)\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:03.731266Z","iopub.execute_input":"2025-01-03T20:31:03.732412Z","iopub.status.idle":"2025-01-03T20:31:06.721299Z","shell.execute_reply.started":"2025-01-03T20:31:03.732369Z","shell.execute_reply":"2025-01-03T20:31:06.720223Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"discard_missing_row = missing_row_report(test, missrow=12)\ntest = test.drop(discard_missing_row)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:11.606087Z","iopub.execute_input":"2025-01-03T20:31:11.606511Z","iopub.status.idle":"2025-01-03T20:31:11.666286Z","shell.execute_reply.started":"2025-01-03T20:31:11.606477Z","shell.execute_reply":"2025-01-03T20:31:11.665231Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"test = test.drop(discard_missing_col, axis=1)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:15.110626Z","iopub.execute_input":"2025-01-03T20:31:15.111113Z","iopub.status.idle":"2025-01-03T20:31:15.130170Z","shell.execute_reply.started":"2025-01-03T20:31:15.111067Z","shell.execute_reply":"2025-01-03T20:31:15.128731Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"test = missing_imputer(X_train, test)[1]\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:18.867279Z","iopub.execute_input":"2025-01-03T20:31:18.867710Z","iopub.status.idle":"2025-01-03T20:31:19.134908Z","shell.execute_reply.started":"2025-01-03T20:31:18.867676Z","shell.execute_reply":"2025-01-03T20:31:19.133818Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(48707, 23)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"test = transform_features(test)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:22.641374Z","iopub.execute_input":"2025-01-03T20:31:22.641786Z","iopub.status.idle":"2025-01-03T20:31:23.147179Z","shell.execute_reply.started":"2025-01-03T20:31:22.641749Z","shell.execute_reply":"2025-01-03T20:31:23.146110Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"(48707, 25)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"predictions = pipe.predict(test)\n\ntest.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:25.862489Z","iopub.execute_input":"2025-01-03T20:31:25.862912Z","iopub.status.idle":"2025-01-03T20:31:30.068753Z","shell.execute_reply.started":"2025-01-03T20:31:25.862875Z","shell.execute_reply":"2025-01-03T20:31:30.067775Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(48707, 25)"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"test = test.reset_index(drop=True)  # Resets index and removes the old index column\n\ntest.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:34.391141Z","iopub.execute_input":"2025-01-03T20:31:34.392328Z","iopub.status.idle":"2025-01-03T20:31:34.407360Z","shell.execute_reply.started":"2025-01-03T20:31:34.392272Z","shell.execute_reply":"2025-01-03T20:31:34.406234Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(48707, 25)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Create the submission DataFrame\nsubmission_df = pd.DataFrame(data={'RefId': test.index, 'IsBadBuy': predictions})\n\n# Save the submission file\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file 'submission.csv' created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T20:31:40.883432Z","iopub.execute_input":"2025-01-03T20:31:40.883841Z","iopub.status.idle":"2025-01-03T20:31:40.930911Z","shell.execute_reply.started":"2025-01-03T20:31:40.883804Z","shell.execute_reply":"2025-01-03T20:31:40.929758Z"}},"outputs":[{"name":"stdout","text":"Submission file 'submission.csv' created successfully!\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}